python inference_attention.py \
       --pretrain_epochs 4 \
       --learning_rate 0.0003 --epochs 20 --hidden_size 512 \
       --train_batch_size 16 --test_batch_size 8 \
       --test ../../data/dev_200.tsv \
       --working_dir visualization_dir\
       --debias_weight 1.3 --pointer_generator --bert_encoder --bert_full_embeddings \
       --tagging_pretrain_epochs 3 --token_softmax --extra_features_top --pre_enrich --activation_hidden --freeze_tagger
